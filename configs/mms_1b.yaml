# @package _group_

common:
  _name: null
  no_progress_bar: false
  log_interval: 200
  log_format: json
  log_file: null
  tensorboard_logdir: tb
  wandb_project: null
  azureml_logging: false
  seed: 1
  cpu: false
  tpu: false
  bf16: false
  memory_efficient_bf16: false
  fp16: true
  memory_efficient_fp16: True
  fp16_no_flatten_grads: false
  fp16_init_scale: 128
  fp16_scale_window: null
  fp16_scale_tolerance: 0.0
  min_loss_scale: 0.0001
  threshold_loss_scale: null
  user_dir: null
  empty_cache_freq: 0
  all_gather_list_size: 16384
  model_parallel_size: 1
  quantization_config_path: null
  profile: false
  reset_logging: false
  suppress_crashes: false
  use_plasma_view: false
  plasma_path: /tmp/plasma

checkpoint:
  save_interval_updates: 25000
  keep_interval_updates: 1
  no_epoch_checkpoints: true

task:
  _name: audio_pretraining
  data: /fsx/arbabu/bible/split_manifests2/inter_alpha0_5_everything/
  labels: null
  sample_rate: 16000
  normalize: true
  enable_padding: false
  max_sample_size: 320000
  min_sample_size: 32000
  num_batch_buckets: 0
  tpu: false

dataset:
  batch_size: 4
  num_workers: 6
  max_tokens: 1200000
  skip_invalid_size_inputs_valid_test: true

distributed_training:
  distributed_world_size: 128
  ddp_backend: legacy_ddp

criterion:
  _name: wav2vec
  infonce: true
  loss_weights:
  - 0.1
  - 0.0
  log_keys:
  - prob_perplexity
  - code_perplexity
  - temp

optimization:
  _name: null
  max_epoch: 0
  max_update: 1000000
  stop_time_hours: 0.0
  clip_norm: 0.3
  sentence_avg: false
  update_freq:
  - 1
  lr:
  - 0.00015
  stop_min_lr: -1.0
  use_bmuf: false

optimizer:
  _name: adam
  adam_betas: (0.9,0.98)
  adam_eps: 1.0e-06
  weight_decay: 0.01
  use_old_adam: false
  tpu: false
  lr:
  - 0.00015

lr_scheduler:
  _name: polynomial_decay
  warmup_updates: 800
  force_anneal: null
  end_learning_rate: 0.0
  power: 1.0
  total_num_update: 1000000
  lr:
  - 0.00015

model:
  _name: wav2vec2
  extractor_mode: layer_norm
  encoder_layers: 48
  encoder_embed_dim: 1280
  encoder_ffn_embed_dim: 5120
  encoder_attention_heads: 16
  activation_fn: gelu
  dropout: 0.0
  attention_dropout: 0.0
  activation_dropout: 0.0
  encoder_layerdrop: 0.0
  dropout_input: 0.1
  dropout_features: 0.1
  final_dim: 1024
  layer_norm_first: true
  conv_feature_layers: '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]'
  conv_bias: true
  logit_temp: 0.1
  quantize_targets: true
  quantize_input: false
  same_quantizer: false
  target_glu: false
  feature_grad_mult: 1.0
  latent_vars: 320
  latent_groups: 2
  latent_dim: 0
  checkpoint_activations: false
  mask_length: 10
  mask_prob: 0.65
  mask_selection: static
  mask_other: 0.0
  no_mask_overlap: false
  mask_min_space: 1
  mask_channel_length: 10
  mask_channel_prob: 0.0
  mask_channel_selection: static
  mask_channel_other: 0.0
  no_mask_channel_overlap: false
  mask_channel_min_space: 1
  num_negatives: 100
  negatives_from_everywhere: false
  cross_sample_negatives: 0
  codebook_negatives: 0
  conv_pos: 128
  conv_pos_groups: 16
  latent_temp:
  - 2.0
  - 0.1
  - 0.999995